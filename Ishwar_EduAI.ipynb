{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Student Performance Feedback System**"
      ],
      "metadata": {
        "id": "HfsS_AmQgvlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet openai fpdf matplotlib pandas gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQf_cLHr59mp",
        "outputId": "48350d5e-bb5b-49e6-c758-2d3252aac55d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from openai import OpenAI\n",
        "from fpdf import FPDF\n",
        "import re\n",
        "from datetime import datetime\n",
        "import textwrap\n",
        "from collections import deque\n",
        "import tempfile\n",
        "import os"
      ],
      "metadata": {
        "id": "HGiprGlMVE3j"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_validate_data(file):\n",
        "    if file is None:\n",
        "        raise ValueError(\"No file uploaded!\")\n",
        "\n",
        "    try:\n",
        "        with open(file.name, 'r') as f:\n",
        "            submission = json.load(f)\n",
        "\n",
        "        if isinstance(submission, list):\n",
        "            submission = submission[0]\n",
        "\n",
        "        required = ['totalTimeTaken', 'totalMarkScored', 'totalAttempted',\n",
        "                    'totalCorrect', 'accuracy', 'subjects', 'sections']\n",
        "\n",
        "        for field in required:\n",
        "            if field not in submission:\n",
        "                raise ValueError(f\"Missing required field: {field}\")\n",
        "\n",
        "        return submission\n",
        "    except json.JSONDecodeError:\n",
        "        raise ValueError(\"Invalid JSON file format\")\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Error processing file: {str(e)}\")\n",
        "\n",
        "def process_subject_data(subjects):\n",
        "    return pd.DataFrame([{\n",
        "        \"Subject\": s['subjectId']['$oid'],\n",
        "        \"Accuracy (%)\": round(s['accuracy'], 2),\n",
        "        \"Attempts\": s.get('attempted', 0),\n",
        "        \"Correct\": s.get('correct', 0)\n",
        "    } for s in subjects])\n",
        "\n",
        "def process_chapter_data(sections):\n",
        "    chapters = {}\n",
        "    for sec in sections:\n",
        "        for q in sec['questions']:\n",
        "            chs = q['questionId'].get('chapters', [])\n",
        "            if not chs:\n",
        "                continue\n",
        "\n",
        "            name = chs[0]['title']\n",
        "            entry = chapters.setdefault(name, {\"attempted\": 0, \"correct\": 0, \"time\": 0, \"marks\": 0})\n",
        "\n",
        "            if q.get('status') in ['answered', 'answeredAndMarkedReview']:\n",
        "                entry['attempted'] += 1\n",
        "                entry['time'] += q.get('timeTaken', 0)\n",
        "\n",
        "                if q.get('inputValue', {}).get('isCorrect'):\n",
        "                    entry['correct'] += 1\n",
        "                    entry['marks'] += q.get('marks', 1)\n",
        "\n",
        "    return pd.DataFrame([{\n",
        "        \"Chapter\": k,\n",
        "        \"Accuracy (%)\": round(v['correct'] / v['attempted'] * 100, 2) if v['attempted'] else 0,\n",
        "        \"Avg Time (s)\": round(v['time'] / v['attempted'], 2) if v['attempted'] else 0,\n",
        "        \"Marks Obtained\": v['marks'],\n",
        "        \"Marks/Question\": round(v['marks'] / v['correct'], 2) if v['correct'] else 0,\n",
        "        \"Attempts\": v['attempted'],\n",
        "        \"Correct\": v['correct']\n",
        "    } for k, v in chapters.items()])\n",
        "\n",
        "def create_visualizations(subjects_df, chapter_df):\n",
        "    plt.style.use('ggplot')\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('Student Performance Analysis Dashboard', fontsize=18, y=0.98, fontweight='bold')\n",
        "\n",
        "    subjects_sorted = subjects_df.sort_values('Accuracy (%)', ascending=False)\n",
        "    bars1 = axs[0,0].bar(subjects_sorted['Subject'], subjects_sorted['Accuracy (%)'],\n",
        "                         color='#4C72B0', alpha=0.8)\n",
        "    axs[0,0].set_ylim(0, 100)\n",
        "    axs[0,0].set_title(\"Subject-wise Accuracy\", fontsize=14, fontweight='bold')\n",
        "    axs[0,0].set_ylabel(\"Accuracy (%)\")\n",
        "    axs[0,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    for bar in bars1:\n",
        "        height = bar.get_height()\n",
        "        axs[0,0].text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "                     f'{height:.1f}%', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "    chapter_top = chapter_df.sort_values('Accuracy (%)', ascending=False).head(10)\n",
        "    bars2 = axs[0,1].bar(range(len(chapter_top)), chapter_top['Accuracy (%)'],\n",
        "                         color='#55A868', alpha=0.8)\n",
        "    axs[0,1].set_xticks(range(len(chapter_top)))\n",
        "    axs[0,1].set_xticklabels([name[:15] + '...' if len(name) > 15 else name\n",
        "                             for name in chapter_top['Chapter']], rotation=45, ha='right')\n",
        "    axs[0,1].set_title(\"Top 10 Chapters by Accuracy\", fontsize=14, fontweight='bold')\n",
        "    axs[0,1].set_ylabel(\"Accuracy (%)\")\n",
        "\n",
        "    scatter = axs[1,0].scatter(chapter_df['Avg Time (s)'], chapter_df['Accuracy (%)'],\n",
        "                              color='#C44E52', s=60, alpha=0.7)\n",
        "    axs[1,0].set_xlabel('Average Time per Question (seconds)')\n",
        "    axs[1,0].set_ylabel('Accuracy (%)')\n",
        "    axs[1,0].set_title(\"Time vs Accuracy Analysis\", fontsize=14, fontweight='bold')\n",
        "    axs[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "    chapter_marks = chapter_df.sort_values('Marks Obtained', ascending=False).head(10)\n",
        "    bars3 = axs[1,1].bar(range(len(chapter_marks)), chapter_marks['Marks Obtained'],\n",
        "                         color='#8172B2', alpha=0.8)\n",
        "    axs[1,1].set_xticks(range(len(chapter_marks)))\n",
        "    axs[1,1].set_xticklabels([name[:15] + '...' if len(name) > 15 else name\n",
        "                             for name in chapter_marks['Chapter']], rotation=45, ha='right')\n",
        "    axs[1,1].set_title(\"Top 10 Chapters by Marks Obtained\", fontsize=14, fontweight='bold')\n",
        "    axs[1,1].set_ylabel(\"Marks Obtained\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def format_time_duration(seconds):\n",
        "    hours = seconds // 3600\n",
        "    minutes = (seconds % 3600) // 60\n",
        "    secs = seconds % 60\n",
        "\n",
        "    if hours > 0:\n",
        "        return f\"{hours}h {minutes}m {secs}s\"\n",
        "    elif minutes > 0:\n",
        "        return f\"{minutes}m {secs}s\"\n",
        "    else:\n",
        "        return f\"{secs}s\"\n",
        "\n",
        "def clean_and_format_text(text):\n",
        "    text = re.sub(r'\\*\\*\\*([^*]+)\\*\\*\\*', r'\\1', text)\n",
        "    text = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', text)\n",
        "    text = re.sub(r'\\*([^*]+)\\*', r'\\1', text)\n",
        "    text = re.sub(r'###\\s*([^\\n]+)', r'\\1', text)\n",
        "    text = re.sub(r'##\\s*([^\\n]+)', r'\\1', text)\n",
        "    text = re.sub(r'#\\s*([^\\n]+)', r'\\1', text)\n",
        "\n",
        "    replacements = {\n",
        "        \"\"\": '\"', \"\"\": '\"', \"'\": \"'\", \"'\": \"'\",\n",
        "        \"â€™\": \"'\", \"â€œ\": '\"', \"â€\": '\"',\n",
        "        \"Ã¢â‚¬â„¢\": \"'\", \"Ã¢â‚¬Å\": '\"',\n",
        "        \"youâ€™ve\": \"you've\", \"youâ€™re\": \"you're\",\n",
        "        \"donâ€™t\": \"don't\", \"canâ€™t\": \"can't\",\n",
        "        \"wonâ€™t\": \"won't\", \"itâ€™s\": \"it's\",\n",
        "\n",
        "        \"–\": \"-\", \"—\": \"-\", \"…\": \"...\",\n",
        "\n",
        "        \"•\": \"- \", \"◦\": \"  - \", \"▪\": \"- \",\n",
        "        \"★\": \"*\", \"✓\": \"v\", \"✗\": \"x\",\n",
        "        \"\\u2022\": \"- \", \"\\u2023\": \"- \",\n",
        "        \"\\u2024\": \"- \", \"\\u2025\": \"- \",\n",
        "        \"\\u2026\": \"...\", \"\\u2013\": \"-\", \"\\u2014\": \"-\",\n",
        "        \"\\u201c\": '\"', \"\\u201d\": '\"',\n",
        "        \"\\u2018\": \"'\", \"\\u2019\": \"'\",\n",
        "\n",
        "        \"▸\": \"- \", \"▹\": \"- \", \"▶\": \"- \",\n",
        "        \"►\": \"- \", \"⟶\": \"->\", \"→\": \"->\",\n",
        "    }\n",
        "\n",
        "    for k, v in replacements.items():\n",
        "        text = text.replace(k, v)\n",
        "\n",
        "    text = re.sub(r\"(\\w)â€™(\\w)\", r\"\\1'\\2\", text)\n",
        "    text = re.sub(r\"(\\w)â€(\\w)\", r\"\\1'\\2\", text)\n",
        "\n",
        "    text = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', text)\n",
        "    text = re.sub(r' +', ' ', text)\n",
        "    text = text.strip()\n",
        "\n",
        "    paragraphs = []\n",
        "    for para in text.split('\\n\\n'):\n",
        "        para = para.strip()\n",
        "        if para:\n",
        "            para = re.sub(r'^[-*+]\\s+', '- ', para, flags=re.MULTILINE)\n",
        "            para = re.sub(r'^\\d+\\.\\s+', lambda m: f\"{m.group().strip()} \", para, flags=re.MULTILINE)\n",
        "\n",
        "            wrapped = textwrap.fill(para, width=85, break_long_words=False, break_on_hyphens=False)\n",
        "            paragraphs.append(wrapped)\n",
        "\n",
        "    final_text = '\\n\\n'.join(paragraphs)\n",
        "\n",
        "    final_text = final_text.encode('ascii', 'ignore').decode('ascii')\n",
        "\n",
        "    final_text = re.sub(r'[^\\x00-\\x7F]+', ' ', final_text)\n",
        "\n",
        "    return final_text\n",
        "\n",
        "def compose_feedback_prompt(student_data, subjects_df, chapter_df):\n",
        "    \"\"\"Compose a detailed prompt for LLM feedback generation\"\"\"\n",
        "    time_formatted = format_time_duration(student_data['totalTimeTaken'])\n",
        "\n",
        "    subjects_sorted = subjects_df.sort_values('Accuracy (%)', ascending=False)\n",
        "    top_subjects = subjects_sorted.head(3)['Subject'].tolist()\n",
        "    weak_subjects = subjects_sorted.tail(3)['Subject'].tolist()\n",
        "\n",
        "    chapters_sorted = chapter_df.sort_values('Accuracy (%)', ascending=False)\n",
        "    top_chapters = chapters_sorted.head(5)['Chapter'].tolist()\n",
        "    weak_chapters = chapters_sorted.tail(5)['Chapter'].tolist()\n",
        "\n",
        "    return f\"\"\"\n",
        "You are an experienced educational mentor providing personalized feedback to a student.\n",
        "\n",
        "STUDENT'S TEST PERFORMANCE SUMMARY:\n",
        "- Total Time Taken: {time_formatted}\n",
        "- Total Marks Scored: {student_data['totalMarkScored']}\n",
        "- Questions Attempted: {student_data['totalAttempted']}\n",
        "- Correct Answers: {student_data['totalCorrect']}\n",
        "- Overall Accuracy: {student_data['accuracy']:.1f}%\n",
        "\n",
        "STRONGEST SUBJECTS: {', '.join(top_subjects)}\n",
        "SUBJECTS NEEDING IMPROVEMENT: {', '.join(weak_subjects)}\n",
        "\n",
        "TOP PERFORMING CHAPTERS: {', '.join(top_chapters)}\n",
        "CHAPTERS NEEDING ATTENTION: {', '.join(weak_chapters)}\n",
        "\n",
        "DETAILED PERFORMANCE DATA:\n",
        "Subject-wise Performance:\n",
        "{subjects_df.to_string(index=False)}\n",
        "\n",
        "Chapter-wise Performance (Top 15):\n",
        "{chapter_df.head(15).to_string(index=False)}\n",
        "\n",
        "Please provide a comprehensive, personalized feedback report with the following structure:\n",
        "\n",
        "1. MOTIVATIONAL OPENING\n",
        "   - Acknowledge their effort and participation\n",
        "   - Highlight overall performance positively\n",
        "\n",
        "2. DETAILED PERFORMANCE ANALYSIS\n",
        "   - Break down subject-wise strengths and areas for improvement\n",
        "   - Discuss chapter-wise performance patterns\n",
        "   - Analyze time management efficiency\n",
        "\n",
        "3. KEY INSIGHTS & OBSERVATIONS\n",
        "   - Identify learning patterns from the data\n",
        "   - Point out any notable trends or correlations\n",
        "\n",
        "4. ACTIONABLE IMPROVEMENT STRATEGIES\n",
        "   - Provide 3-4 specific, practical strategies\n",
        "   - Focus on both content mastery and test-taking skills\n",
        "\n",
        "5. PERSONALIZED STUDY PLAN\n",
        "   - Suggest a structured approach for weak areas\n",
        "   - Include time allocation recommendations\n",
        "\n",
        "6. ENCOURAGING CONCLUSION\n",
        "   - Reinforce confidence and growth mindset\n",
        "   - Set positive expectations for future performance\n",
        "\n",
        "Write in a warm, encouraging tone as if you're speaking directly to the student. Make it personal and actionable.\n",
        "IMPORTANT: Use only basic ASCII characters in your response. Avoid special bullets, fancy quotes, or Unicode symbols.\n",
        "\"\"\"\n",
        "\n",
        "def generate_llm_feedback(prompt):\n",
        "    try:\n",
        "        client = OpenAI(\n",
        "            base_url=\"https://openrouter.ai/api/v1\",\n",
        "            api_key=\"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\"\n",
        "        )\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"deepseek/deepseek-chat-v3-0324:free\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            extra_headers={\n",
        "                \"HTTP-Referer\": \"https://test.com\",\n",
        "                \"X-Title\": \"StudentFeedback\"\n",
        "            },\n",
        "            max_tokens=2000,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        fallback_feedback = f\"\"\"Dear Student,\n",
        "\n",
        "I apologize, but I'm unable to generate personalized AI feedback at this moment due to a technical issue: {str(e)}\n",
        "\n",
        "However, based on your performance data, here are some general observations:\n",
        "\n",
        "PERFORMANCE SUMMARY:\n",
        "- Your overall accuracy is showing room for improvement\n",
        "- Focus on consistent practice and understanding concepts thoroughly\n",
        "\n",
        "GENERAL RECOMMENDATIONS:\n",
        "1. Review your incorrect answers to understand mistake patterns\n",
        "2. Practice time management during tests\n",
        "3. Focus extra attention on weaker subject areas\n",
        "4. Create a regular study schedule\n",
        "5. Seek help from teachers for challenging topics\n",
        "\n",
        "Keep up the good work and stay motivated!\n",
        "\n",
        "Best regards,\n",
        "Your Study Assistant\"\"\"\n",
        "\n",
        "        return fallback_feedback\n",
        "\n",
        "class generatePDF(FPDF):\n",
        "    def header(self):\n",
        "        self.set_fill_color(41, 128, 185)\n",
        "        self.rect(0, 0, 210, 25, 'F')\n",
        "\n",
        "        self.set_text_color(255, 255, 255)\n",
        "        self.set_font('Arial', 'B', 20)\n",
        "        self.cell(0, 15, 'Student Performance Analysis Report', ln=1, align='C', border=0)\n",
        "\n",
        "        self.set_font('Arial', '', 12)\n",
        "        self.cell(0, 8, f\"Generated on: {datetime.now().strftime('%B %d, %Y at %I:%M %p')}\",\n",
        "                 ln=1, align='C', border=0)\n",
        "\n",
        "        self.set_text_color(0, 0, 0)\n",
        "        self.ln(10)\n",
        "\n",
        "    def footer(self):\n",
        "        self.set_y(-15)\n",
        "        self.set_font('Arial', 'I', 10)\n",
        "        self.set_text_color(128, 128, 128)\n",
        "        self.cell(0, 10, f'Page {self.page_no()} | Student Performance Report', align='C')\n",
        "\n",
        "    def add_section_header(self, title, color=(52, 73, 94)):\n",
        "        self.ln(5)\n",
        "        self.set_fill_color(*color)\n",
        "        self.set_text_color(255, 255, 255)\n",
        "        self.set_font('Arial', 'B', 14)\n",
        "        self.cell(0, 12, f\"  {title}\", ln=1, fill=True)\n",
        "        self.set_text_color(0, 0, 0)\n",
        "        self.ln(3)\n",
        "\n",
        "    def add_formatted_text(self, text, font_size=11, line_height=6):\n",
        "        text = clean_and_format_text(text)\n",
        "\n",
        "        self.set_font('Arial', '', font_size)\n",
        "\n",
        "        paragraphs = text.strip().split('\\n\\n')\n",
        "\n",
        "        for i, para in enumerate(paragraphs):\n",
        "            if not para.strip():\n",
        "                continue\n",
        "\n",
        "            is_header = False\n",
        "            if (len(para) < 60 and\n",
        "                (para.isupper() or\n",
        "                 re.match(r'^\\d+\\.?\\s+[A-Z]', para.strip()) or\n",
        "                 para.strip().endswith(':'))):\n",
        "                is_header = True\n",
        "\n",
        "            if is_header:\n",
        "                self.ln(3)\n",
        "                self.set_font('Arial', 'B', font_size + 1)\n",
        "                self.set_text_color(41, 128, 185)\n",
        "            else:\n",
        "                self.set_font('Arial', '', font_size)\n",
        "                self.set_text_color(0, 0, 0)\n",
        "\n",
        "            lines = para.split('\\n')\n",
        "            for line in lines:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "\n",
        "                if line.startswith('- '):\n",
        "                    self.cell(8, line_height, '', 0, 0)\n",
        "                    bullet_text = line[2:].strip()\n",
        "\n",
        "                    try:\n",
        "                        self.multi_cell(0, line_height, f\"* {bullet_text}\")\n",
        "                    except UnicodeEncodeError:\n",
        "                        cleaned_bullet = bullet_text.encode('ascii', 'ignore').decode('ascii')\n",
        "                        self.multi_cell(0, line_height, f\"* {cleaned_bullet}\")\n",
        "                elif re.match(r'^\\d+\\.', line):\n",
        "                    try:\n",
        "                        self.multi_cell(0, line_height, line)\n",
        "                    except UnicodeEncodeError:\n",
        "                        cleaned_line = line.encode('ascii', 'ignore').decode('ascii')\n",
        "                        self.multi_cell(0, line_height, cleaned_line)\n",
        "                else:\n",
        "                    try:\n",
        "                        self.multi_cell(0, line_height, line)\n",
        "                    except UnicodeEncodeError:\n",
        "                        cleaned_line = line.encode('ascii', 'ignore').decode('ascii')\n",
        "                        self.multi_cell(0, line_height, cleaned_line)\n",
        "\n",
        "            if i < len(paragraphs) - 1:\n",
        "                self.ln(2)\n",
        "\n",
        "    def add_data_table(self, title, df, max_rows=10):\n",
        "        self.add_section_header(title, (46, 125, 50))\n",
        "\n",
        "        self.set_font('Arial', 'B', 10)\n",
        "        self.set_fill_color(240, 240, 240)\n",
        "\n",
        "        col_widths = [60, 30, 30, 30, 30]\n",
        "        headers = df.columns.tolist()[:5]\n",
        "\n",
        "        for i, header in enumerate(headers):\n",
        "            self.cell(col_widths[i] if i < len(col_widths) else 30, 8, str(header), 1, 0, 'C', True)\n",
        "        self.ln()\n",
        "\n",
        "        self.set_font('Arial', '', 9)\n",
        "        for idx, row in df.head(max_rows).iterrows():\n",
        "            for i, col in enumerate(headers):\n",
        "                value = str(row[col])\n",
        "                if len(value) > 20:\n",
        "                    value = value[:17] + \"...\"\n",
        "\n",
        "                try:\n",
        "                    self.cell(col_widths[i] if i < len(col_widths) else 30, 6, value, 1, 0, 'C')\n",
        "                except UnicodeEncodeError:\n",
        "                    cleaned_value = value.encode('ascii', 'ignore').decode('ascii')\n",
        "                    self.cell(col_widths[i] if i < len(col_widths) else 30, 6, cleaned_value, 1, 0, 'C')\n",
        "            self.ln()\n",
        "\n",
        "def create_pdf_report(viz_path, student_data, subjects_df, chapter_df, feedback_text):\n",
        "    pdf = generatePDF()\n",
        "    pdf.add_page()\n",
        "\n",
        "    pdf.add_section_header(\"Performance Summary\", (52, 152, 219))\n",
        "\n",
        "    summary_text = f\"\"\"\n",
        "Overall Accuracy: {student_data['accuracy']:.1f}%\n",
        "Total Marks Scored: {student_data['totalMarkScored']}\n",
        "Questions Attempted: {student_data['totalAttempted']} out of total questions\n",
        "Correct Answers: {student_data['totalCorrect']}\n",
        "Time Taken: {format_time_duration(student_data['totalTimeTaken'])}\n",
        "Average Time per Question: {student_data['totalTimeTaken'] // student_data['totalAttempted'] if student_data['totalAttempted'] > 0 else 0} seconds\n",
        "\"\"\"\n",
        "\n",
        "    pdf.add_formatted_text(summary_text)\n",
        "\n",
        "    if viz_path and os.path.exists(viz_path):\n",
        "        pdf.add_section_header(\"Performance Visualizations\", (231, 76, 60))\n",
        "        try:\n",
        "            pdf.image(viz_path, x=10, y=None, w=190)\n",
        "            pdf.ln(10)\n",
        "        except:\n",
        "            pdf.add_formatted_text(\"Visualization could not be embedded in the PDF.\")\n",
        "\n",
        "    pdf.add_data_table(\"Subject-wise Performance\", subjects_df)\n",
        "    pdf.ln(5)\n",
        "    pdf.add_data_table(\"Chapter-wise Performance (Top 10)\", chapter_df.head(10))\n",
        "\n",
        "    pdf.add_page()\n",
        "    pdf.add_section_header(\"Personalized AI Feedback\", (155, 89, 182))\n",
        "\n",
        "    # Extra cleaning for feedback text\n",
        "    cleaned_feedback = clean_and_format_text(feedback_text)\n",
        "    pdf.add_formatted_text(cleaned_feedback, font_size=11, line_height=6)\n",
        "\n",
        "    output_path = \"student_performance_report.pdf\"\n",
        "    pdf.output(output_path)\n",
        "    return output_path\n",
        "\n",
        "def process_student_data(file, progress=gr.Progress()):\n",
        "    if file is None:\n",
        "        return \"Please upload a JSON file first.\", None, None, None\n",
        "\n",
        "    try:\n",
        "        progress(0.1, desc=\"Loading and validating data...\")\n",
        "        student_data = load_and_validate_data(file)\n",
        "\n",
        "        progress(0.3, desc=\"Processing subject and chapter data...\")\n",
        "        subjects_df = process_subject_data(student_data['subjects'])\n",
        "        chapter_df = process_chapter_data(student_data['sections'])\n",
        "\n",
        "        progress(0.5, desc=\"Creating visualizations...\")\n",
        "        fig = create_visualizations(subjects_df, chapter_df)\n",
        "\n",
        "        viz_path = tempfile.mktemp(suffix='.png')\n",
        "        fig.savefig(viz_path, bbox_inches='tight', dpi=300, facecolor='white')\n",
        "        plt.close()\n",
        "\n",
        "        progress(0.7, desc=\"Generating AI feedback...\")\n",
        "        prompt = compose_feedback_prompt(student_data, subjects_df, chapter_df)\n",
        "        feedback = generate_llm_feedback(prompt)\n",
        "\n",
        "        progress(0.9, desc=\"Creating PDF report...\")\n",
        "        report_path = create_pdf_report(viz_path, student_data, subjects_df, chapter_df, feedback)\n",
        "\n",
        "        progress(1.0, desc=\"Complete!\")\n",
        "\n",
        "        summary = f\"\"\"\n",
        "## Analysis Complete!\n",
        "\n",
        "**Performance Summary:**\n",
        "- Overall Accuracy: {student_data['accuracy']:.1f}%\n",
        "- Total Marks: {student_data['totalMarkScored']}\n",
        "- Questions Attempted: {student_data['totalAttempted']}\n",
        "- Time Taken: {format_time_duration(student_data['totalTimeTaken'])}\n",
        "\n",
        "**Top Performing Subject:** {subjects_df.loc[subjects_df['Accuracy (%)'].idxmax(), 'Subject']} ({subjects_df['Accuracy (%)'].max():.1f}%)\n",
        "\n",
        "**Best Chapter:** {chapter_df.loc[chapter_df['Accuracy (%)'].idxmax(), 'Chapter']} ({chapter_df['Accuracy (%)'].max():.1f}%)\n",
        "\n",
        "Download your detailed PDF report below!\n",
        "\"\"\"\n",
        "\n",
        "        return summary, viz_path, report_path, feedback\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\", None, None, None\n",
        "\n",
        "def create_gradio_interface():\n",
        "    with gr.Blocks(title=\"Student Performance Analyzer\", theme=gr.themes.Soft()) as interface:\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        #  Student Performance Analysis Dashboard\n",
        "\n",
        "        Upload your test results JSON file to get a comprehensive performance analysis with AI-powered feedback!\n",
        "\n",
        "        ### Features:\n",
        "        -  **Visual Analytics**: Subject and chapter-wise performance charts\n",
        "        -  **AI Feedback**: Personalized insights and study recommendations\n",
        "        -  **PDF Report**: Professional downloadable report\n",
        "        -  **Time Analysis**: Time vs accuracy correlations\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### Upload Test Results\")\n",
        "                file_input = gr.File(\n",
        "                    label=\"Upload JSON File\",\n",
        "                    file_types=[\".json\"],\n",
        "                    type=\"filepath\"\n",
        "                )\n",
        "\n",
        "                analyze_btn = gr.Button(\n",
        "                    \"Analyze Performance\",\n",
        "                    variant=\"primary\",\n",
        "                    size=\"lg\"\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"### 📋 Analysis Results\")\n",
        "                summary_output = gr.Markdown(label=\"Summary\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                gr.Markdown(\"### Performance Visualizations\")\n",
        "                chart_output = gr.Image(label=\"Performance Charts\")\n",
        "\n",
        "            with gr.Column():\n",
        "                gr.Markdown(\"### Download Report\")\n",
        "                pdf_output = gr.File(label=\"PDF Report\")\n",
        "\n",
        "        with gr.Row():\n",
        "            gr.Markdown(\"### AI-Generated Feedback\")\n",
        "            feedback_output = gr.Textbox(\n",
        "                label=\"Personalized Feedback\",\n",
        "                lines=15,\n",
        "                max_lines=20,\n",
        "                show_copy_button=True\n",
        "            )\n",
        "\n",
        "        analyze_btn.click(\n",
        "            fn=process_student_data,\n",
        "            inputs=[file_input],\n",
        "            outputs=[summary_output, chart_output, pdf_output, feedback_output],\n",
        "            show_progress=True\n",
        "        )\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        ### Instructions:\n",
        "        1. Upload a JSON file containing your test results\n",
        "        2. Click \"Analyze Performance\" to process the data\n",
        "        3. View your performance visualizations and AI feedback\n",
        "        4. Download the comprehensive PDF report\n",
        "\n",
        "        ### Supported Format:\n",
        "        Your JSON file should contain fields like: `totalTimeTaken`, `totalMarkScored`, `subjects`, `sections`, etc.\n",
        "        \"\"\")\n",
        "\n",
        "    return interface\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    interface = create_gradio_interface()\n",
        "    interface.launch(\n",
        "        share=True,\n",
        "        show_error=True,\n",
        "        inbrowser=True\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "CAE_NpOP6AtD",
        "outputId": "17945e0d-aa1b-49dc-ce45-87a215185764"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://3f1c8c7abf0f2d2fc9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3f1c8c7abf0f2d2fc9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}